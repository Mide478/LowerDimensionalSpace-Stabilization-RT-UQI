{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "### Stabilized Representations in Lower Dimensional Space\n",
    "\n",
    "\n",
    "#### Midé Mabadeje$^{1}$ (PhD Candidate) & Michael Pyrcz$^{1,2}$ (Associate Professor), University of Texas at Austin\n",
    "\n",
    " 1. Hildebrand Department of Petroleum and Geosystems Engineering, Cockrell School of Engineering\n",
    "\n",
    " 2. Department of Geological Sciences, Jackson School of Geosciences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stabilized Representations in Lower Dimensional Space\n",
    "\n",
    "Here's a demonstration on how to find the optimal rotation, reflection, and translation for corresponding points in a low dimensional space to help stabilize the projections such that the solutions obtained are invariant when using multidimensional scaling (MDS) as a dimensionality reduction method and other Manifold DR methods.\n",
    "\n",
    "\n",
    "#### Multidimensional Scaling\n",
    "\n",
    "A powerful ordination method in inferential statistics / information visualization for exploring / visualizing the similarity (conversely the difference) between individual samples from a high dimensional dataset.\n",
    "\n",
    "* beyond 2 or 3 features it is difficult to visualize the relationship between samples\n",
    "\n",
    "* for 2 features we can easily visualize the relationships between samples with a scatter plot\n",
    "\n",
    "* for 3 features we can either visualize in 3D or include color or matrix scatter plots\n",
    "\n",
    "Multidimensional scaling projects the $m$ dimensional data to $p$ dimensions such that $p << m$.\n",
    "\n",
    "* ideally we are able to project to $p=2$ to easily explore the relationships between the samples\n",
    "\n",
    "While principal component analysis (PCA) operates with the covariance matrix, multidimensional scaling operates with the distance / dissimilarity matrix.\n",
    "\n",
    "* you don't need to know the actual feature values, just the distance or dissimilarity between the samples\n",
    "\n",
    "* as with any distance in feature space, we consider feature standardization and weighting\n",
    "\n",
    "* we may also work with a variety of dissimilarity measures\n",
    "\n",
    "\n",
    "#### Metric Multidimensional Scaling\n",
    "\n",
    "A generalization of classical multidimensional scaling with a variety of metrics and a loss function optimization.\n",
    "\n",
    "* formulated as an optimization problem to minimize the square difference between the original and projected pairwise distances\n",
    "\n",
    "\\begin{equation}\n",
    "min_{x_1,\\ldots,x_m} \\sum_{i<j} \\left( ||x_i - x_j|| - \\delta_{i,j} \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "where $||x_i - x_j||$ are the pairwise distances in the projected space ($p$ dimensional) and $\\delta_{i,j}$ are the pairwise distances in the original feature space.\n",
    "\n",
    "\n",
    "General comments about metric multidimensional scaling:\n",
    "\n",
    "* nonlinear dimensionality reduction\n",
    "\n",
    "* no distribution assumption\n",
    "\n",
    "* dissimilarity measure must be meaningful\n",
    "\n",
    "* dimensionality reduction is performed such that the error in the sample pairwise distance is minimized\n",
    "\n",
    "* there is a variant known as Non-metric Multidimensional Scaling for ordinal features (categorical with ordering).\n",
    "\n",
    "#### Checking Multidimensional Scaling Results\n",
    "\n",
    "The multidimensional scaling approach minimizes the square difference of the pairwise distances between all the data samples and each other between the projected, lower dimensional, and original feature space.\n",
    "\n",
    "* **stress** is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Stress_P(x_1,\\ldots,x_n) = \\left( \\sum_{i \\ne j = 1,\\ldots,n} \\left( ||x_i - x_j|| - \\delta_{i,j} \\right)^2 \\right)^{\\frac{1}{2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "However, the above is the raw stress value, which is not very informative as high values does not necessarily indicate bad fit. A better way of communicating reliability is to calculate a normed stress, e.g. with Stress-1 implemented according to Kruskal (1964) on p. 3 where 0 indicates a perfect fit, 0.025 excellent, 0.05 good, 0.1 fair, and 0.2 poor. For more information see Kruskal (1964) p. 8–9 and Borg (2005) p.41–43\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Stress_{norm} (x_1,\\ldots,x_n) = \\left( \\dfrac { \\sum_{i \\ne j = 1,\\ldots,n} \\left( ||x_i - x_j|| - \\delta_{i,j} )^2 \\right } \\left\\sum_{i \\ne j = 1,\\ldots,n}\\left( \\delta_{i,j}^2 \\right)\\right \\right)^{\\frac{1}{2}}\n",
    "\\end{equation}\n",
    "\n",
    "where $||x_i - x_j||$ are the pairwise distances in the projected space ($p$ dimensional) and $\\delta_{i,j}$ are the pairwise distances in the original feature space.\n",
    "\n",
    "\n",
    "* it is also useful to visualize the scatter plot of projected vs. original pairwise distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rigid Transformations\n",
    "\n",
    "A rigid transformation also known as isometry or Euclidean transformation is a transformation of the plane that preserves length. Reflections, translations, rotations, and combinations of these three transformations are \"rigid transformations\".\n",
    "\n",
    "1. A translation is a transformation which \"slides\" a figure a fixed distance in a given direction without changing its size or shape, and without turning and flipping it.\n",
    "\n",
    "2. A rotation is a transformation that turns a figure about a fixed point called the center of rotation. An object and its rotation are the same shape and size, but the figures may be turned in different directions. Rotations may be clockwise or counterclockwise.\n",
    "\n",
    "3. A reflection can be thought of as folding or \"flipping\" an object over the line of reflection A point reflection exists when a figure is built around a single point called the center of the figure, or point of reflection.  For every point in the figure, there is another point found directly opposite it on the other side of the center such that the point of reflection becomes the midpoint of the segment joining the point with its image.  Under a point reflection, figures do not change size or shape.\n",
    "\n",
    "However, in the case of MDS projections from high dimensional datasets to low dimensions these transformations are common place individually or combinatorially."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/).\n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal.\n",
    "3. In the terminal type: pip install geostatspy.\n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality.\n",
    "\n",
    "You will need to copy the data file to your working directory.  They are available here:\n",
    "\n",
    "* Tabular data - unconv_MV_v4.csv at https://git.io/fhHLT.\n",
    "\n",
    "There are examples below with GeostatsPy functions. You can go here to see a list of the available functions, https://git.io/fh4eX, and for other example workflows and source code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Autoresampling within 95% CI and class implementation for N-case and OOSP case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np                        # ndarray for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import os                                 # set working directory, run executables\n",
    "import seaborn as sns                     # for matrix scatter plots\n",
    "# import RigidTransformation_UQI as RT      # imports script consisting of functions to run workflow\n",
    "import TestScript as RT      # imports script consisting of functions to run workflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_ipython()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the working directory\n",
    "\n",
    "I always like to do this so, I don't lose files and to simplify subsequent read and writes (avoid including the full address each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:28.952106Z",
     "start_time": "2022-11-17T21:33:28.940308Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd # grab current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:29.129511Z",
     "start_time": "2022-11-17T21:33:29.125052Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/ademidemabadeje/Documents/UT/Research/PyCharm/LD_Stabilization/Fall 2022/Results/Current Notebook')    # set the working directory to results directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Tabular Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:29.799528Z",
     "start_time": "2022-11-17T21:33:29.499477Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv')\n",
    "df['TOC'] = np.where(df['TOC']<0.0, 0.0, df['TOC']) # set TOC < 0.0 as 0.0, otherwise leave the same\n",
    "df.head()                             # we could also use this command for a table preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has features from 200 unconventional wells including: \n",
    "\n",
    "0. well index\n",
    "1. well average porosity (%)\n",
    "2. permeability (mD)\n",
    "3. accoustic impedance (kg/m2s*10^6)\n",
    "4. brittness ratio (%) \n",
    "5. total organic carbon (%) \n",
    "6. vitrinite reflectance (%)\n",
    "8. normalized initial production 90 day average (MCFPD). \n",
    "\n",
    "Note, the dataset is synthetic, but has realistic ranges and general multivariate relationships.\n",
    "\n",
    "Ranking features is really an effort to understand the features and their relationships with eachother.  We will start with basic data visualization and move to more complicated methods such are partial correlation and recursive feature elimination.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Summary Statistics\n",
    "\n",
    "Let's check the summary statistics of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:30.581807Z",
     "start_time": "2022-11-17T21:33:30.535594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics are a critical first step in data checking. \n",
    "\n",
    "* this includes the number of valid (non-null) values for each feature (count removes all np.NaN from the totals for each variable).\n",
    "\n",
    "* we can see the general behaviours such as central tendency, mean, and dispersion, variance.\n",
    "\n",
    "* we can identify issue with negative values, extreme values, and values that are outside the range of plausible values for each property. \n",
    "\n",
    "* We can also establish the feature ranges for plotting.  We could calculate the feature range directly from the data with code like this:\n",
    "\n",
    "```p\n",
    "Pormin = np.min(df['Por'].values)          # extract ndarray of data table column\n",
    "Pormax = np.max(df['Por'].values)          # and calculate min and max\n",
    "```\n",
    "\n",
    "but, this would not result in easy to understand color bars and axis scales, let's pick convenient round numbers. We will also declare feature labels for ease of plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:31.158614Z",
     "start_time": "2022-11-17T21:33:31.149051Z"
    }
   },
   "outputs": [],
   "source": [
    "pormin = 6.0; pormax = 24.0; porname = 'Porosity (%)'; portitle = 'Porosity' # user specified min and max values, and labels for plotting\n",
    "permmin = 0.0; permmax = 10; permname = 'Permeability (mD)'; permtitle = 'Permeability'                \n",
    "AImin = 1.0; AImax = 5.0; AIname = 'Acoustic Impedance (kg/m2s*10^6)'; AItitle = 'Acoustic Impedance'\n",
    "brmin = 10.0; brmax = 85.0; brname = 'Brittleness Ratio (%)'; brtitle = 'Brittleness'\n",
    "TOCmin = 0.0; TOCmax = 2.2; TOCname = 'Total Organic Carbon (%)'; TOCtitle = 'Total Organic Carbon' \n",
    "VRmin = 0.9; VRmax = 2.9; VRname = 'Vitrinite Reflectance (%)'; VRtitle = 'Vitrinite Reflectance'\n",
    "prodmin = 500.0; prodmax = 9000.0; prodname = 'Normalized Initial Production (MCFPD)'; prodtitle = 'Normalized Initial Production'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the data looks to be in pretty good shape from its summary statistics and for brevity we skip outlier detection since synthetic i.e., a toy dataset. Let's look at the distributions with a martix scatter plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.copy(deep=True)\n",
    "df.rename(columns={'Por': 'PHI (%)','Perm': 'K (mD)', 'VR': 'VR (%)', 'Brittle': 'BR (%)',\n",
    "                       'TOC': 'TOC (%)','AI': 'AI (kgm2/s)', 'Prod': 'Np (MCFPD)'}, inplace=True)\n",
    "\n",
    "RT.matrix_scatter(df, ['PHI (%)', 'K (mD)', 'VR (%)', 'BR (%)', 'TOC (%)', 'AI (kgm2/s)',\n",
    "                'Np (MCFPD)'], 0., 0., 1., 0.6, 0.4, 0.4, 'Scatterplot of all features for entire data', 1, hue_=None, n_case=True, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## save data frame, it is needed for reruns input\n",
    "df.to_csv('/Users/ademidemabadeje/Documents/UT/Research/PyCharm/LD_Stabilization/Fall 2022/Data/to_use.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = 'Np label'\n",
    "num_response = 'Np (MCFPD)'\n",
    "\n",
    "# Add category for response variable i.e., production levels for complete dataset\n",
    "df = RT.make_levels(data=df, cat_response=response, num_response=num_response)\n",
    "RT.matrix_scatter(df, ['PHI (%)', 'K (mD)',  'VR (%)', 'BR (%)', 'TOC (%)', 'AI (kgm2/s)',\n",
    "                'Np (MCFPD)'], 0., 0., 1., 0.6, 0.4, 0.4, 'Scatterplot of all features coded by production levels for entire dataset', 1, hue_='Np label', n_case=True, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Curate parameters and variables for the proposed workflow i.e., N case\n",
    "N = 30\n",
    "df_subset = df.iloc[:N,1:-1]\n",
    "\n",
    "# Curate variables for N+1 case by adding a new sample i.e., row to existing dataframe within 95% CI from the dataframe in the N case and obtain random seed used to add the OOSP.\n",
    "df_subset2, random_seed_used = RT.make_sample_within_ci(df_subset.copy())\n",
    "\n",
    "# Insert well column index back into data frame for N+1 case\n",
    "df_subset2.insert(0, 'Well', np.arange(1, len(df_subset2)+1))\n",
    "\n",
    "# Insert well column index back into data frame for N case\n",
    "df_subset.insert(0, 'Well', np.arange(1, len(df_subset)+1))\n",
    "\n",
    "# Select predictor features of interest for proposed workflow\n",
    "features = ['PHI (%)', 'AI (kgm2/s)', 'TOC (%)']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Let's make an ordinal feature from the continuous production:\n",
    "\n",
    "1. low\n",
    "2. medium\n",
    "3. high\n",
    "4. very high\n",
    "\n",
    "production rates.  This will help us visualize the results as we proceed, we can look at wells with different levels of production projected into a variety of lower dimensional spaces with multidimensional scaling.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's start with the N-samples case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset for use in N case inclusive of category for response variable i.e., production levels\n",
    "df_subset = RT.make_levels(data=df_subset, cat_response=response, num_response=num_response)\n",
    "df_subset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at the matrix scatter plot of our 3 features and the production levels for N case samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the N-case dataset\n",
    "\n",
    "other_df = RT.standardizer(df_subset, features, keep_only_std_features=False)\n",
    "\n",
    "# Visualize the matrix scatter plot of the 3 standardized features and the response i.e., production levels.\n",
    "RT.matrix_scatter(other_df, ['NS_PHI (%)', 'NS_AI (kgm2/s)', 'NS_TOC (%)'], 0.0, 0.0, 1.5, 1.5, 0.3, 0.2,\n",
    "                  'Scatterplot of standardized features colored by production levels for N case samples', 1, hue_=response, n_case=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the N-case dataset\n",
    "\n",
    "# Visualize the matrix scatter plot of the 3 features chosen and the response i.e., production levels in the original space.\n",
    "RT.matrix_scatter(other_df, ['PHI (%)', 'AI (kgm2/s)', 'TOC (%)'], 0.0, 0.0, 1.5, 1.5, 0.3, 0.2,\n",
    "                  'Scatterplot of features colored by production levels for N case samples', 1, hue_=response, n_case=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, do the same for the N+1 samples case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset for use in N+1 case inclusive of category for response variable i.e., production levels\n",
    "df_subset2 = RT.make_levels(data=df_subset2, cat_response=response, num_response=num_response)\n",
    "df_subset2.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the matrix scatter plot of our 3 features and the production levels for N+1 case samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the N+1 case dataset\n",
    "\n",
    "other_df = RT.standardizer(df_subset2, features, keep_only_std_features=False)\n",
    "\n",
    "# Visualize the matrix scatter plot of the 3 standardized features and the response i.e., production levels.\n",
    "#RT.\n",
    "RT.matrix_scatter(other_df, ['NS_PHI (%)', 'NS_AI (kgm2/s)', 'NS_TOC (%)'], 0.0, 0.0, 1.5, 1.5, 0.3, 0.2,\n",
    "                  'Scatterplot of standardized features colored by production levels for N+1 case samples', 1, hue_=response, n_case=False, save=True)\n",
    "\n",
    "# r'NS_AI  ($\\mathrm{kg \\, m^2/s}$)' for label in latex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the N+1 case dataset\n",
    "\n",
    "# Visualize the matrix scatter plot of the 3 standardized features and the response i.e., production levels.\n",
    "#RT.\n",
    "RT.matrix_scatter(other_df, ['PHI (%)', 'AI (kgm2/s)', 'TOC (%)'], 0.0, 0.0, 1.5, 1.5, 0.3, 0.2,\n",
    "                  'Scatterplot of features colored by production levels for N+1 case samples', 1, hue_=response, n_case=False, save=True)\n",
    "\n",
    "# r'NS_AI  ($\\mathrm{kg \\, m^2/s}$)' for label in latex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Workflow\n",
    "\n",
    "#### Multidimensional Scaling\n",
    "\n",
    "The multidimensional scaling method follows the sample pattern as other scikit-learn methods, we instantiate, fit and then apply or transform.\n",
    "\n",
    "Let's run multidimensional scaling on our subset of features ($m = 3$) and project to only 2 features ($p = 2$).\n",
    "\n",
    "* we set the random_state for repeatability, everyone gets the same result from the iterative solution\n",
    "\n",
    "* we use 20 random initializations, the best solution is selected to improve likelihood of selection of (or search resulting in) the global optimum and not a local optimum\n",
    "\n",
    "* we use an increased number of max_iter to improve the convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: N-case samples\n",
    "\n",
    "\n",
    "To make the above problem more trackable and due to computational power consumption, we highlight the use of the proposed workflow.\n",
    "\n",
    "First, we take a subset of the entire dataset, e.g., 30 samples from the original dataset, then instantiate the class. Do note that standardization is performed on the features under the hood, this is imperative as our model works with dissimilarity-based distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Curate parameters and variables\n",
    "num_realizations = 100\n",
    "base_seed = 42\n",
    "start_seed = 1\n",
    "stop_seed = 10000\n",
    "idx = 'Well'\n",
    "dissimilarity_metric='euclidean'\n",
    "make_figure = True\n",
    "dim_projection = '2d' # since the projection (p), is to 2 features. Use '3d' if the projection is to 3 features\n",
    "normalize_projections = False\n",
    "\n",
    "# Instantiate the class\n",
    "obj = RT.RigidTransformation(df=df_subset, features=features, idx=idx, num_realizations=num_realizations, base_seed=base_seed, start_seed=start_seed, stop_seed=stop_seed, dissimilarity_metric=dissimilarity_metric, dim_projection=dim_projection)\n",
    "\n",
    "# Run rigid MDS\n",
    "random_seeds, all_real, calc_real, all_rmse, norm_stress = obj.run_rigid_MDS(normalize_projections=normalize_projections)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the base case and three realizations using the same data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response='Np label'\n",
    "r_list=[0, 10, 40, 80]\n",
    "Ax='MDS 1'\n",
    "Ay='MDS 2'\n",
    "title=['Base case realization ', 'Realization ', 'Realization ', 'Realization ']\n",
    "x_off=0.02\n",
    "y_off=0.07\n",
    "\n",
    "#Set color map criteria\n",
    "dpalette = sns.color_palette(\"rocket_r\",n_colors = len(df[response].unique())+1)\n",
    "cmap=dpalette\n",
    "\n",
    "obj.real_plotter(response=response, r_idx=r_list, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=None, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prior to stabilization of data points, let's visualize all realizations on the same plot to show that MDS is not rotation invariant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.bivariate_plotter(palette_=1, response=response, x_off=x_off, y_off=y_off, title='Rotational variation in MDS for all realizations',\n",
    "                     plot_type='variation', Ax=Ax, Ay=Ay, annotate=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we visualize the base case, previously chosen realizations and the stabilized solution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.real_plotter(response=response, r_idx=r_list, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=calc_real, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the 2D registration jitters in the calculated stabilized solution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.bivariate_plotter(palette_=1, response=response, x_off=x_off, y_off=y_off, title='2-D registration distortion in N-case stabilized solutions',\n",
    "                         plot_type='jitters', Ax=Ax, Ay=Ay, annotate=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.bivariate_plotter(palette_=1, response=response, x_off=x_off, y_off=y_off, title='2-D registration distortion uncertainty in N-case stabilized solutions',\n",
    "                         plot_type='uncertainty', Ax=Ax, Ay=Ay, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the scenarios above, we see that regardless of the seed or random state used when computing the MDS projections, similar stabilized solutions are obtained with distortions/jitters. Next, we average out the stabilized solutions obtained to get a more accurate location for the projections in the MDS space by obtaining the expectation of the ith sample over enough realizations as the ground truth projection coordinates.\n",
    "\n",
    "\n",
    "Note that using the ensemble expectation of the stabilized solution helps removed effects from slight random offsets, perturbations, or distortions if any within the n-sample demonstration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set base case index of choice\n",
    "bc_idx =0\n",
    "\n",
    "\n",
    "E = obj.expectation(r_idx=bc_idx, Ax=Ax, Ay=Ay, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the ensemble expectation of the stabilized MDS projections over multiple realizations compared to the base case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xmin = -5\n",
    "xmax = 5\n",
    "ymin = 0.0\n",
    "ymax = 0.45\n",
    "\n",
    "obj.expect_plotter(r_idx=bc_idx, Lx=Ax, Ly=Ay, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case with ensemble expectation of the stabilized solution over all realizations for comparison."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.compare_plot(response=response, r_idx=bc_idx, Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, cmap=dpalette, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples. NEED TO UNDERSTAND INTERPRETATION OF STRESS HERE!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.visual_model_check(norm_type='L1', fig_name='Model check for N case samples distortion', array=E, expectation_compute=True, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above, We projected to a 2 dimensional feature space and did a pretty good job preserving the pairwise distances between the samples.\n",
    "\n",
    "\n",
    "Now, let's find the convex hull polygon and identify the anchor points for the n-case scenario in the lower dimensional space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points, hull, vertices = obj.convex_hull(array=all_real, title='N sample case', x_off=0.025, y_off=0.03, Ax=Ax, Ay=Ay, make_figure=make_figure, expectation_compute=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the marginal distributions for each predictor and threshold at which proposed methodology will work and/or fail. This is shown because if the OOSP is chosen from the tails, the vertices of the anchor set might change, making stabilization a non-trivial task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj.marginal_dbn(save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# Section 2:  For N + 1 Case\n",
    "<br>\n",
    "\n",
    "Let's add a sample to the data yielding a N+1 samples and re-run the proposed workflow using N+1 samples on previous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "obj2 = RT.RigidTransf_NPlus(df=df_subset2, features=features, idx=idx, num_realizations=num_realizations, base_seed=base_seed, start_seed=start_seed, stop_seed=stop_seed, dissimilarity_metric='euclidean', dim_projection=dim_projection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run rigid MDS\n",
    "random_seeds2, all_real2, calc_real2, all_rmse2, norm_stress2 = obj2.run_rigid_MDS(normalize_projections = normalize_projections)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find convex hull polygon"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points2, hull2, vertices2 = obj2.convex_hull(array=all_real2, title='N+1 sample case', x_off=0.025, y_off=0.03, Ax=Ax, Ay=Ay, make_figure=make_figure, expectation_compute=False, n_case=False,save=True) #0.05,0.015"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we stabilize the anchors from the N+1 case to match the N-case such that the same representation is obtained and visualize them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors1, anchors2, R_anchors, t_anchors, rmse_err_anchors, stable_coords_anchors, stable_coords_alldata, rmse_err_alldata = obj2.stabilize_anchors(array1=my_points, array2=my_points2, hull_1=hull, hull_2=hull2, normalize_projections=False)\n",
    "\n",
    "obj2.stable_anchor_visuals(Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.stable_representation(title='Stabilized N+1 case', Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, sample_added=len(df_subset2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.visual_model_check(norm_type='L1', fig_name='Model check for N+1 case samples distortion', array=stable_coords_alldata, expectation_compute=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the 2D registration jitters in n+1 case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.bivariate_plotter(palette_=1, response=response, x_off=x_off, y_off=y_off, title='2-D registration distortion in N+1 case stabilized solutions',\n",
    "                         plot_type='jitters', Ax=Ax, Ay=Ay, annotate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.bivariate_plotter(palette_=1, response=response, x_off=x_off, y_off=y_off, title='2-D registration distortion in N+1-case stabilized solutions',\n",
    "                         plot_type='uncertainty', Ax=Ax, Ay=Ay, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to section 2, but uses the expected stabilized solution for N+1 scenario to get the anchor points then compare to N-case.\n",
    "\n",
    "<br>\n",
    "\n",
    "Are there significant changes or is it just more computationally intensive?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case and three realizations for N+1 case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.real_plotter(response=response, r_idx=r_list, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=None, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case, previously chosen realizations, and stabilized solution for N+1 scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.real_plotter(response=response, r_idx=r_list, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=calc_real2, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the scenarios above, we see that regardless of the seed or random state used when computing the MDS projections, the similar stabilized solutions are obtained. Next, we average out the stabilized solutions obtained to get a more accurate location for the projections in the MDS space by obtaining the expectation of the ith sample over enough realizations as the ground truth projection coordinates.\n",
    "\n",
    "\n",
    "Note that using the ensemble expectation of the stabilized solution helps removed effects from slight random offsets, perturbations, or distortions if any within the n+1-sample demonstration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set base case index of choice for N+1 scenario\n",
    "bc_idx =0 # new_idx\n",
    "\n",
    "\n",
    "E2 = obj2.expectation(r_idx=bc_idx, Ax=Ax, Ay=Ay, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the ensemble expectation of the stabilized MDS projections for n+1 samples over multiple realizations compared to the base case from using new_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xmin2 = -5\n",
    "xmax2 = 5\n",
    "ymin2 = 0.0\n",
    "ymax2 = 0.45\n",
    "\n",
    "obj2.expect_plotter(r_idx=bc_idx, Lx=Ax, Ly=Ay, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the base case with ensemble expectation of the stabilized solution over all realizations for comparison to view distortions if any"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.compare_plot(response=response, r_idx=bc_idx, Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, cmap=dpalette, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the convex hull polygon of the expected stabilized solution for all realizations in the n+1 scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points_expected, hull_expected, vertices_expected = obj2.convex_hull(array=E2, title='Ensemble Expectation of N+1 sample case', x_off=0.025, y_off=0.03, Ax=Ax, Ay=Ay, make_figure=make_figure, expectation_compute=True, n_case=False, save=True) #0.01,0.015"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find stabilized expected anchor representation and visualize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors1_exp, anchors2_exp, R_anchors_exp, t_anchors_exp, rmse_err_anchors_exp, stable_coords_anchors_exp, stable_coords_alldata_exp, rmse_err_alldata_exp = obj2.stabilize_anchors(array1=my_points, array2=my_points_expected, hull_1=hull, hull_2=hull_expected, normalize_projections=normalize_projections)\n",
    "\n",
    "obj2.stable_anchor_visuals( Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.stable_representation(title='Stabilized Ensemble Expectation for N+1 case', Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, sample_added=len(df_subset2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj2.visual_model_check(norm_type='L1', fig_name='Model check for the expectation of N+1 case samples distortion', array=E2, expectation_compute=True, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The average, minimum, and maximum stress values for the realizations in the n-scenario are \", round(np.mean(norm_stress),5) , round(np.min(norm_stress),5)\n",
    "      , round(np.max(norm_stress),5))\n",
    "\n",
    "print(\"The average, minimum, and maximum stress values for the realizations in the n+1 -scenario are \", round(np.mean(norm_stress2),5) , round(np.min(norm_stress2),5)\n",
    "      , round(np.max(norm_stress2),5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T05:43:35.412108Z",
     "start_time": "2022-08-18T05:43:35.403071Z"
    }
   },
   "source": [
    "## Ademide (Midé) Mabadeje, Ph.D.Candidate, University of Texas at Austin \n",
    "\n",
    "\n",
    "Midé is currently working on Geostatistics – spatial data analytics, where she is creating novel workflows and data-driven algorithms to debias and reduce uncertainty in subsurface prediction models. Her Ph.D. research revolves around creating new data analytics and ML workflows to address various sources of spatial and/or subsurface bias to obtain representative sampling to impact decision-making and economic development.\n",
    "\n",
    "For more about Midé check out these links:\n",
    "\n",
    "####  [GitHub](https://github.com/Mide478) | [GoogleScholar](https://scholar.google.com/citations?user=9nksjzQAAAAJ&hl=en&oi=ao)  | [LinkedIn](https://www.linkedin.com/in/ademidemabadeje)\n",
    "\n",
    "\n",
    "I am open to internships and collaborations in related research areas and disciplines underlined with spatial statistics. I can be reached at ademidemabadeje@austin.utexas.edu.\n",
    "\n",
    "## Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](https://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "We hope this was helpful,\n",
    "\n",
    "*Midé & Michael*\n",
    "\n",
    "\n",
    "\n",
    "Midé Mabadeje, Graduate Research Assistant The Hildebrand Department of Petroleum and Geosystems Engineering, The University of Texas at Austin\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
