{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "### Stabilized Representations in Lower Dimensional Space\n",
    "\n",
    "\n",
    "#### Midé Mabadeje$^{1}$ (PhD Candidate) & Michael Pyrcz$^{1,2}$ (Associate Professor), University of Texas at Austin\n",
    "\n",
    " 1. Hildebrand Department of Petroleum and Geosystems Engineering, Cockrell School of Engineering\n",
    "\n",
    "\n",
    " 2. Department of Geological Sciences, Jackson School of Geosciences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stabilized Representations in Lower Dimensional Space\n",
    "\n",
    "Here's a demonstration on how to find the optimal rotation, reflection, and translation for corresponding points in a low dimensional space to help stabilize the projections such that the solutions obtained are invariant when using multidimensional scaling (MDS) as a dimensionality reduction method and other Manifold DR methods.\n",
    "\n",
    "\n",
    "#### Multidimensional Scaling\n",
    "\n",
    "A powerful ordination method in inferential statistics / information visualization for exploring / visualizing the similarity (conversely the difference) between individual samples from a high dimensional dataset.\n",
    "\n",
    "* beyond 2 or 3 features it is difficult to visualize the relationship between samples\n",
    "\n",
    "* for 2 features we can easily visualize the relationships between samples with a scatter plot\n",
    "\n",
    "* for 3 features we can either visualize in 3D or include color or matrix scatter plots\n",
    "\n",
    "Multidimensional scaling projects the $m$ dimensional data to $p$ dimensions such that $p << m$.\n",
    "\n",
    "* ideally we are able to project to $p=2$ to easily explore the relationships between the samples\n",
    "\n",
    "While principal component analysis (PCA) operates with the covariance matrix, multidimensional scaling operates with the distance / dissimilarity matrix.\n",
    "\n",
    "* you don't need to know the actual feature values, just the distance or dissimilarity between the samples\n",
    "\n",
    "* as with any distance in feature space, we consider feature standardization and weighting\n",
    "\n",
    "* we may also work with a variety of dissimilarity measures\n",
    "\n",
    "\n",
    "#### Metric Multidimensional Scaling\n",
    "\n",
    "A generalization of classical multidimensional scaling with a variety of metrics and a loss function optimization.\n",
    "\n",
    "* formulated as an optimization problem to minimize the square difference between the original and projected pairwise distances\n",
    "\n",
    "\\begin{equation}\n",
    "min_{x_1,\\ldots,x_m} \\sum_{i<j} \\left( ||x_i - x_j|| - \\delta_{i,j} \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "where $||x_i - x_j||$ are the pairwise distances in the projected space ($p$ dimensional) and $\\delta_{i,j}$ are the pairwise distances in the original feature space.\n",
    "\n",
    "\n",
    "General comments about metric multidimensional scaling:\n",
    "\n",
    "* nonlinear dimensionality reduction\n",
    "\n",
    "* no distribution assumption\n",
    "\n",
    "* dissimilarity measure must be meaningful\n",
    "\n",
    "* dimensionality reduction is performed such that the error in the sample pairwise distance is minimized\n",
    "\n",
    "* there is a variant known as Non-metric Multidimensional Scaling for ordinal features (categorical with ordering).\n",
    "\n",
    "#### Checking Multidimensional Scaling Results\n",
    "\n",
    "The multidimensional scaling approach minimizes the square difference of the pairwise distances between all of the data samples and eachother between the projected, lower dimensional, and original feature space.\n",
    "\n",
    "* **stress** is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Stress_P(x_1,\\ldots,x_n) = \\left( \\sum_{i \\ne j = 1,\\ldots,n} \\left( ||x_i - x_j|| - \\delta_{i,j} \\right)^2 \\right)^{\\frac{1}{2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "However, the above is the raw stress value, which is not very informative as high values does not necessarily indicate bad fit. A better way of communicating reliability is to calculate a normed stress, e.g. with Stress-1 implemented according to Kruskal (1964) on p. 3 where 0 indicates a perfect fit, 0.025 excellent, 0.05 good, 0.1 fair, and 0.2 poor. For more information see Kruskal (1964) p. 8–9 and Borg (2005) p.41–43\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Stress_{norm} (x_1,\\ldots,x_n) = \\left( \\dfrac { \\sum_{i \\ne j = 1,\\ldots,n} \\left( ||x_i - x_j|| - \\delta_{i,j} )^2 \\right } \\left\\sum_{i \\ne j = 1,\\ldots,n}\\left( \\delta_{i,j}^2 \\right)\\right \\right)^{\\frac{1}{2}}\n",
    "\\end{equation}\n",
    "\n",
    "where $||x_i - x_j||$ are the pairwise distances in the projected space ($p$ dimensional) and $\\delta_{i,j}$ are the pairwise distances in the original feature space.\n",
    "\n",
    "\n",
    "* it is also useful to visualize the scatter plot of projected vs. original pairwise distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rigid Transformations\n",
    "\n",
    "A rigid transformation also known as isometry or Euclidean transformation is a transformation of the plane that preserves length. Reflections, translations, rotations, and combinations of these three transformations are \"rigid transformations\".\n",
    "\n",
    "1. A translation is a transformation which \"slides\" a figure a fixed distance in a given direction without changing its size or shape, and without turning and flipping it.\n",
    "\n",
    "2. A rotation is a transformation that turns a figure about a fixed point called the center of rotation. An object and its rotation are the same shape and size, but the figures may be turned in different directions. Rotations may be clockwise or counterclockwise.\n",
    "\n",
    "3. A reflection can be thought of as folding or \"flipping\" an object over the line of reflection A point reflection exists when a figure is built around a single point called the center of the figure, or point of reflection.  For every point in the figure, there is another point found directly opposite it on the other side of the center such that the point of reflection becomes the midpoint of the segment joining the point with its image.  Under a point reflection, figures do not change size or shape.\n",
    "\n",
    "However, in the case of MDS projections from high dimensional datasets to low dimensions these transformations are common place individually or combinatorially."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/).\n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal.\n",
    "3. In the terminal type: pip install geostatspy.\n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality.\n",
    "\n",
    "You will need to copy the data file to your working directory.  They are available here:\n",
    "\n",
    "* Tabular data - unconv_MV_v4.csv at https://git.io/fhHLT.\n",
    "\n",
    "There are examples below with GeostatsPy functions. You can go here to see a list of the available functions, https://git.io/fh4eX, and for other example workflows and source code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Functions as fn                    # imports script consisting of functions to run workflow\n",
    "import numpy as np                        # ndarray for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import joypy\n",
    "import os                                 # set working directory, run executables\n",
    "import matplotlib.pyplot as plt           # for plotting\n",
    "import seaborn as sns                     # for matrix scatter plots\n",
    "from shapely.geometry import Polygon, Point\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the working directory\n",
    "\n",
    "I always like to do this so, I don't lose files and to simplify subsequent read and writes (avoid including the full address each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:28.952106Z",
     "start_time": "2022-11-17T21:33:28.940308Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:29.129511Z",
     "start_time": "2022-11-17T21:33:29.125052Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/ademidemabadeje/Documents/UT/Research/PyCharm/LD_Stabilization/Fall 2022/Data')                     # set the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Tabular Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:29.799528Z",
     "start_time": "2022-11-17T21:33:29.499477Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv')\n",
    "df['TOC'] = np.where(df['TOC']<0.0, 0.0, df['TOC']) # set TOC < 0.0 as 0.0, otherwise leave the same\n",
    "df.head()                             # we could also use this command for a table preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has features from 200 unconventional wells including: \n",
    "\n",
    "0. well index\n",
    "1. well average porosity (%) \n",
    "2. permeability (mD)\n",
    "3. accoustic impedance (kg/m2s*10^6)\n",
    "4. brittness ratio (%) \n",
    "5. total organic carbon (%) \n",
    "6. vitrinite reflectance (%)\n",
    "8. normalized initial production 90 day average (MCFPD). \n",
    "\n",
    "Note, the dataset is synthetic, but has realistic ranges and general multivariate relationships.\n",
    "\n",
    "Ranking features is really an effort to understand the features and their relationships with eachother.  We will start with basic data visualization and move to more complicated methods such are partial correlation and recursive feature elimination.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Summary Statistics\n",
    "\n",
    "Let's check the summary statistics of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:30.581807Z",
     "start_time": "2022-11-17T21:33:30.535594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics are a critical first step in data checking. \n",
    "\n",
    "* this includes the number of valid (non-null) values for each feature (count removes all np.NaN from the totals for each variable).\n",
    "\n",
    "* we can see the general behaviours such as central tendency, mean, and dispersion, variance.\n",
    "\n",
    "* we can identify issue with negative values, extreme values, and values that are outside the range of plausible values for each property. \n",
    "\n",
    "* We can also establish the feature ranges for plotting.  We could calculate the feature range directly from the data with code like this:\n",
    "\n",
    "```p\n",
    "Pormin = np.min(df['Por'].values)          # extract ndarray of data table column\n",
    "Pormax = np.max(df['Por'].values)          # and calculate min and max\n",
    "```\n",
    "\n",
    "but, this would not result in easy to understand color bars and axis scales, let's pick convenient round numbers. We will also declare feature labels for ease of plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:31.158614Z",
     "start_time": "2022-11-17T21:33:31.149051Z"
    }
   },
   "outputs": [],
   "source": [
    "pormin = 6.0; pormax = 24.0; porname = 'Porosity (%)'; portitle = 'Porosity' # user specified min and max values, and labels for plotting\n",
    "permmin = 0.0; permmax = 10; permname = 'Permeability (mD)'; permtitle = 'Permeability'                \n",
    "AImin = 1.0; AImax = 5.0; AIname = 'Acoustic Impedance (kg/m2s*10^6)'; AItitle = 'Acoustic Impedance'\n",
    "brmin = 10.0; brmax = 85.0; brname = 'Brittleness Ratio (%)'; brtitle = 'Brittleness'\n",
    "TOCmin = 0.0; TOCmax = 2.2; TOCname = 'Total Organic Carbon (%)'; TOCtitle = 'Total Organic Carbon' \n",
    "VRmin = 0.9; VRmax = 2.9; VRname = 'Vitrinite Reflectance (%)'; VRtitle = 'Vitrinite Reflectance'\n",
    "prodmin = 500.0; prodmax = 9000.0; prodname = 'Normalized Initial Production (MCFPD)'; prodtitle = 'Normalized Initial Production'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the data looks to be in pretty good shape from its summary statistics and for brevity we skip outlier detection since synthetic i.e., a toy dataset. Let's look at the distributions with a martix scatter plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.copy(deep=True)\n",
    "df.rename(columns={'Por': 'PHI (%)','Perm': 'K (mD)', 'VR': 'VR (%)', 'Brittle': 'BR (%)',\n",
    "                       'TOC': 'TOC (%)','AI': 'AI (kgm2/s)', 'Prod': 'Np (MCFPD)'}, inplace=True)\n",
    "\n",
    "fn.matrix_scatter(df, ['PHI (%)', 'K (mD)', 'VR (%)', 'BR (%)', 'TOC (%)', 'AI (kgm2/s)',\n",
    "                'Np (MCFPD)'], 0., 0., 1., 0.6, 0.4, 0.4, 'Scatterplot of all features', 1, hue_=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Standardization\n",
    "\n",
    "Since our model works with dissimilarity-based distance metric, it is imperative to standardize the predictor features to avoid predictor pulling effects due to scale variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3D dataset curation\n",
    "features = ['PHI (%)', 'AI (kgm2/s)', 'TOC (%)']\n",
    "df_std = fn.standardizer(df, features=features, keep_only_std_features=True)\n",
    "df = fn.standardizer(df, features=features, keep_only_std_features=False)\n",
    "\n",
    "# Extract standardized feature columns to a list\n",
    "ns_features = ['NS_PHI (%)', 'NS_AI (kgm2/s)', 'NS_TOC (%)']\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation \n",
    "\n",
    "Let's make an ordinal feature from the continuous production:\n",
    "\n",
    "1. low\n",
    "2. medium\n",
    "3. high\n",
    "4. very high \n",
    "\n",
    "production rates.  This will help us visualize the results as we proceed, we can look at wells with different levels of production projected into a variety of lower dimensional spaces with multidimensional scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:33:40.433472Z",
     "start_time": "2022-11-17T21:33:40.419317Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins = [0,2500,5000,7500,10000]                # assign the production bins (these are the fence posts)\n",
    "labels = ['low', 'med', 'high', 'vhigh']       # assign the labels\n",
    "category = pd.cut(df['Np (MCFPD)'],bins,labels=labels)     # make the 1D array with the labels for our data\n",
    "df['Np label'] = category                                # add the new ordinal production feature to our DataFrames\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the matrix scatter plot of our 3 features and the production levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.matrix_scatter(df, ['PHI (%)', 'K (mD)',  'VR (%)', 'BR (%)', 'TOC (%)', 'AI (kgm2/s)',\n",
    "                'Np (MCFPD)'], 0., 0., 1., 0.6, 0.4, 0.4, 'Scatterplot of all features coded by production levels', 1, hue_='Np label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the matrix scatter plot of the 3 standardized features and the production levels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.matrix_scatter(df, ['NS_PHI (%)', 'NS_AI (kgm2/s)', 'NS_TOC (%)'], 0.0, 0.0, 1.5, 1.5, 0.3, 0.2,\n",
    "                  'Scatterplot of standardized features colored by production levels', 1, hue_='Np label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional Scaling\n",
    "\n",
    "The multidimensional scaling method follows the sample pattern as other scikit-learn methods, we instantiate, fit and then apply or transform.\n",
    "\n",
    "Let's run multidimensional scaling on our subset of features ($m = 3$) and project to only 2 features ($p = 2$).\n",
    "\n",
    "* we set the random_state for repeatability, everyone gets the same result from the iterative solution\n",
    "\n",
    "* we use 20 random initializations, the best solution is selected to improve likelihood of selection of (or search resulting in) the global optimum and not a local optimum\n",
    "\n",
    "* we use an increased number of max_iter to improve the convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: n-case samples\n",
    "\n",
    "\n",
    "To make the above problem more trackable and due to computational power consumption, we highlight the use of the proposed workflow, let's use 30 samples from the original dataset, then run MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = 30\n",
    "df_subset = df.iloc[:n,:]\n",
    "num_realizations = 100\n",
    "base_seed = 42\n",
    "start_seed = 1\n",
    "stop_seed = 10000\n",
    "\n",
    "\n",
    "random_seeds, all_real, calc_real, all_rmse, norm_stress = fn.run_rigid_MDS(df=df_subset, ns_features=ns_features, num_realizations=num_realizations, base_seed=base_seed, start_seed=start_seed, stop_seed=stop_seed, dissimilarity_metric='euclidean')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the base case and three realizations using the same data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 'Well'\n",
    "response='Np label'\n",
    "array1=all_real\n",
    "r_list=[0, 10, 40, 80]\n",
    "bc_idx =0 # base case index\n",
    "Ax='MDS 1'\n",
    "Ay='MDS 2'\n",
    "title=['Base case realization ', 'Realization ', 'Realization ', 'Realization ']\n",
    "x_off=0.02\n",
    "y_off=0.07\n",
    "\n",
    "#Set color map criteria\n",
    "dpalette = sns.color_palette(\"rocket_r\",n_colors = len(df[response].unique()))\n",
    "palette = sns.color_palette(\"rocket\")\n",
    "cmap=dpalette\n",
    "\n",
    "fn.real_plotter(df=df_subset, idx=idx, response=response, array1=array1, r_idx=r_list, random_seeds=random_seeds, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prior to stabilization of data points, let's visualize all realizations on the same plot to show that MDS is not rotation invariant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.bivariate_plotter(array=all_real, palette_=1, response='Np label', title='Rotational variation in MDS for all realizations',\n",
    "                     plot_type='variation',dataframe=df_subset, Ax=Ax, Ay=Ay)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we visualize the base case, previously chosen realizations and the stabilized solution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.real_plotter(df=df_subset, idx=idx, response=response, array1=array1, r_idx=r_list, random_seeds=random_seeds, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=calc_real)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the 2D registration jitters in the calculated stabilized solution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.bivariate_plotter(array=calc_real, palette_=1, response='Np label', title='2-D registration distortion in stabilized solutions',\n",
    "                     plot_type='jitters',dataframe=df_subset, Ax=Ax, Ay=Ay)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the scenarios above, we see that regardless of the seed or random state used when computing the MDS projections, similar stabilized solutions are obtained with distortions/jitters. Next, we average out the stabilized solutions obtained to get a more accurate location for the projections in the MDS space by obtaining the expectation of the ith sample over enough realizations as the ground truth projection coordinates.\n",
    "\n",
    "\n",
    "Note that using the expectation of the stabilized solution helps removed effects from slight random offsets, perturbations, or distortions if any within the n-sample demonstration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "E = fn.expectation(array1=all_real, array2=calc_real, r_idx=bc_idx, Ax=Ax, Ay=Ay, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the expectation of the stabilized MDS projections over multiple realizations compared to the base case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T21:34:46.111325Z",
     "start_time": "2022-11-17T21:34:46.044772Z"
    }
   },
   "outputs": [],
   "source": [
    "xmin = -5\n",
    "xmax = 5\n",
    "ymin = 0.0\n",
    "ymax = 0.45\n",
    "\n",
    "fn.E_plotter(array1=all_real, array_exp=E, r_idx=bc_idx, Lx=Ax, Ly=Ay, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case with expectation of the stabilized solution over all realizations for comparison."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.compare_plot(df=df_subset, idx=idx, response=response, array1=all_real, r_idx=bc_idx, num_realizations=num_realizations, array_exp=E, random_seeds=random_seeds, Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, cmap=dpalette)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples. NEED TO UNDERSTAND INTERPRETATION OF STRESS HERE!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.visual_model_check(df_subset, ns_features,'Model check for n-case samples distortion', E)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above, We projected to a 2 dimensional feature space and did a pretty good job preserving the pairwise distances between the samples.\n",
    "\n",
    "\n",
    "Now, let's find the convex hull polygon and identify the anchor points for the n-case scenario in the lower dimensional space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points, hull, vertices = fn.convex_hull(array=all_real, title='N sample case', x_off=0.025, y_off=0.03, Ax=Ax, Ay=Ay)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the marginal distribution of the predictor features for the response level. This is shown because if the n+1 sample is chosen from the tails, the vertices of the convex polygon i.e., anchor points will change and the data will be impossible to stabilize between the n-case and n+1 case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_other = df_subset.loc[:,['PHI (%)','AI (kgm2/s)', 'TOC (%)','Np label']]\n",
    "\n",
    "fig, ax = joypy.joyplot(df_other, ylim='own', fade=True)\n",
    "fig.suptitle(\"Marginal distribution of predictor features\", y=1.0, size=14)\n",
    "fig.set_size_inches(8,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the conditional distribution of each predictor feature conditioned on the different production levels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = joypy.joyplot(df_other, by='Np label', ylim='own', overlap=2, legend=True, fade=True, linewidth=1)\n",
    "fig.suptitle(\"Predictor features distributions conditioned on production labels\", y=1.0, size=14)\n",
    "fig.set_size_inches(8,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the marginal distributions for each predictor and threshold at which proposed methodology will work and/or fail"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.marginal_dbn(dataframe=df_subset, features=features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Test example to see if Convex checking function is working!\n",
    "# sq_df = pd.DataFrame()\n",
    "# sq_x = [2,10,10,2]\n",
    "# sq_y = [1,1,9,9]\n",
    "# sq_df[\"x\"] = sq_x\n",
    "# sq_df[\"y\"] = sq_y\n",
    "# sq_arr = sq_df.loc[:,[\"x\",\"y\"]].values\n",
    "# sq_hull = ConvexHull(sq_arr)\n",
    "# plt.plot(sq_arr[:,0],sq_arr[:,1],'o')\n",
    "# ver = sq_arr[sq_hull.vertices]\n",
    "# pol = Polygon(ver)\n",
    "# bin = fn.is_convex_polygon(pol) # check if data is a strict convex polygon, if true, perform workflow else, DO WHAT?!\n",
    "# print(bin)\n",
    "# for simp in sq_hull.simplices:\n",
    "#     plt.plot(sq_arr[simp, 0], sq_arr[simp, 1], 'r--') # k-\n",
    "#     plt.fill(sq_arr[sq_hull.vertices, 0], sq_arr[sq_hull.vertices, 1], c='yellow', alpha=0.01)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "# Section 2:  For n+ 1 Case\n",
    "<br>\n",
    "\n",
    "Let's add a sample to the data yielding a n+1 samples and re-run the proposed workflow using n+1 samples on previous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n2 = 31\n",
    "# #View the parameters of the i-th sample added\n",
    "# df.iloc[n2,:]\n",
    "\n",
    "df_subset2 = df.iloc[:n2,:]\n",
    "\n",
    "random_seeds2, all_real2, calc_real2, all_rmse2, norm_stress2 = fn.run_rigid_MDS(df=df_subset2, ns_features=ns_features, num_realizations=num_realizations, base_seed=base_seed, start_seed=start_seed, stop_seed=stop_seed, dissimilarity_metric='euclidean')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points2, hull2, vertices2 = fn.convex_hull(array=all_real2, title='N+1 sample case', x_off=0.025, y_off=0.03, Ax=Ax, Ay=Ay) #0.05,0.015"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we stabilize the anchors from the n+1 case to match the n-case such that the same representation is obtained and visualize them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anchors1, anchors2, R_anchors, t_anchors, rmse_err_anchors, stable_coords_anchors, stable_coords_alldata = fn.stabilize_anchors(array1=my_points, array2=my_points2, hull_1=hull, hull_2=hull2)\n",
    "\n",
    "fn.stable_anchor_visuals(anchors_1=anchors1, anchors_2=anchors2, stable_coords_anchors=stable_coords_anchors, Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.stable_representation(stable_coords_alldata=stable_coords_alldata, title='Stabilized N+1 case', Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, sample_added=n2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_points2_anchors = np.column_stack((my_points2[:, 0],my_points2[:, 1],[0]*len(my_points2)))\n",
    "new_coord_alldata = (R_anchors@np.transpose(my_points2_anchors)) + t_anchors\n",
    "new_coord_alldata_ = np.transpose(new_coord_alldata[:2,:])\n",
    "\n",
    "# Visualize the stabilized n+1 scenario for all samples\n",
    "plt.scatter(new_coord_alldata_[:n2-1,0], new_coord_alldata_[:n2-1,1], marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "plt.scatter(new_coord_alldata_[n2-1,0], new_coord_alldata_[n2-1,1], marker='*', color='k', s=90)\n",
    "for index, label in enumerate(range(1,len(new_coord_alldata_[:,0])+1)):\n",
    "    plt.annotate(label, (new_coord_alldata_[:,0][index]+0.05, new_coord_alldata_[:,1][index]+0.05), size=8, style='italic')\n",
    "plt.title(\"hh\")\n",
    "plt.xlabel(\"MDS 1\")\n",
    "plt.ylabel(\"MDS 2\")\n",
    "plt.show() # Perhaps color by production cmap 'dpalette created before'?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Obtain dataframe with the standardized predictor features\n",
    "actual_data2 = df_subset2[ns_features]\n",
    "\n",
    "# Grab the stabilized solution of n+1 scenario\n",
    "stabilized_expected_proj2 = new_coord_alldata_.copy()\n",
    "\n",
    "# insert  distortion\n",
    "dists2 = euclidean_distances(actual_data2, squared=False).ravel()\n",
    "nonzero2 = dists2 != 0   # select only non-identical samples pairs\n",
    "dists2 = dists2[nonzero2]\n",
    "projected_dists2 = euclidean_distances(stabilized_expected_proj2, squared=False).ravel()[nonzero2]\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(dists2,projected_dists2,c='red',alpha=0.2,edgecolor = 'black')\n",
    "plt.arrow(0,0,200,200,width=0.02,color='black',head_length=0.0,head_width=0.0)\n",
    "plt.xlim(0,15); plt.ylim(0,15)\n",
    "plt.xlabel(\"Pairwise Distance: original space\")\n",
    "plt.ylabel(\"Pairwise Distance: projected space\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 components\")\n",
    "\n",
    "rates2 = projected_dists2 / dists2\n",
    "print(\"Distance Ratio, mean: %0.4f, standard deviation %0.4f.\" % (np.mean(rates2), np.std(rates2)))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(rates2, bins=50, range=(0.5, 1.5),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Distance Ratio: projected / original\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 Components\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(dists2, bins=50, range=(0., 15.),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Pairwise Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Original Data\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(projected_dists2, bins=50, range=(0., 15.),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Pairwise Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 Components\")\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.7, top=2.3, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the 2D registration jitters in n+1 case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(0,len(calc_real2)):\n",
    "    mds1_vec = np.transpose(calc_real2[i][0,:])\n",
    "    mds2_vec = np.transpose(calc_real2[i][1,:])\n",
    "    sns.scatterplot(x=mds1_vec, y=mds2_vec, hue=df_subset2['tProd'], s=60, markers='o', alpha=0.01,\n",
    "                    palette=dpalette,edgecolor=\"black\", legend=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What if we use expected stabilized solution for n+1 scenario to get the anchor points then compare to previous section results to compare, any significant changes or just more computationally intensive?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Parameters are repeated so, I didn't redeclare.\n",
    "# idx = 'Well'\n",
    "# response='tProd'\n",
    "# r_list=[0, 10, 40, 80]\n",
    "# Ax='MDS 1'\n",
    "# Ay='MDS 2'\n",
    "# title=['Base case realization ', 'Realization ', 'Realization ', 'Realization ']\n",
    "# x_off=0.02\n",
    "# y_off=0.07\n",
    "# cmap=dpalette"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case and three realizations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "array2 = all_real2\n",
    "new_idx =0 # base case index for n+1 scenario\n",
    "fn.real_plotter(df=df_subset2, idx=idx, response=response, array1=array2, r_idx=r_list, random_seeds=random_seeds2, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize base case, previously chosen realizations and stabilized solution for n+1 scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fn.real_plotter(df=df_subset2, idx=idx, response=response, array1=array2, r_idx=r_list, random_seeds=random_seeds2, Ax=Ax, Ay=Ay , title=title, x_off=x_off, y_off=y_off,cmap=dpalette, array2=calc_real2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the scenarios above, we see that regardless of the seed or random state used when computing the MDS projections, the similar stabilized solutions are obtained. Next, we average out the stabilized solutions obtained to get a more accurate location for the projections in the MDS space by obtaining the expectation of the ith sample over enough realizations as the ground truth projection coordinates.\n",
    "\n",
    "\n",
    "Note that using the expectation of the stabilized solution helps removed effects from slight random offsets, perturbations, or distortions if any within the n+1-sample demonstration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E2 = fn.expectation(array1=all_real2, array2=calc_real2, r_idx=new_idx, Ax=Ax, Ay=Ay, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the expectation of the stabilized MDS projections for n+1 samples over multiple realizations compared to the base case from using new_idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xmin2 = -5\n",
    "xmax2 = 5\n",
    "ymin2 = 0.0\n",
    "ymax2 = 0.45\n",
    "\n",
    "fn.E_plotter(array1=all_real2, array_exp=E2, r_idx=new_idx, Lx=Ax, Ly=Ay, xmin=xmin2, xmax=xmax2, ymin=ymin2, ymax=ymax2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the base case with expectation of the stabilized solution over all realizations for comparison to view distortions if any"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure random_seeds2 is in the same order\n",
    "fn.compare_plot(df=df_subset2, idx=idx, response=response, array1=all_real2, r_idx=new_idx, num_realizations=num_realizations, array_exp=E2, random_seeds=random_seeds2, Ax=Ax, Ay=Ay, x_off=x_off, y_off=y_off, cmap=dpalette)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the convex hull polygon of the expected stabilized solution for all realizations in the n+1 scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# how to enforce anchor in R2 for n+1 case done! and create a convex hull of the stabilized solution points, ie convex polygon for n+1 case. Set this as an assertion!\n",
    "stabilized_expected_all = np.transpose(E2[:2,:])\n",
    "my_points_expected = stabilized_expected_all.copy()\n",
    "hull_expected = ConvexHull(my_points_expected)\n",
    "\n",
    "# Visualize plots\n",
    "plt.scatter(my_points_expected[:n2-1,0], my_points_expected[:n2-1,1],marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "plt.scatter(my_points_expected[n2-1,0], my_points_expected[n2-1,1],marker='*', s=90, color='k', edgecolors=\"black\")\n",
    "for index, label in enumerate(range(1,len(my_points_expected[:,0])+1)):\n",
    "    plt.annotate(label, (my_points_expected[:,0][index]+0.01, my_points_expected[:,1][index]+0.015), size=8, style='italic')\n",
    "\n",
    "# Check for point in polygon\n",
    "vertices_expected = my_points_expected[hull_expected.vertices]\n",
    "# plt.scatter(my_points_expected[hull_expected.vertices][:,0], my_points_expected[hull_expected.vertices][:,1])\n",
    "polygon_expected = Polygon(vertices_expected)\n",
    "\n",
    "binary_bool_expected = fn.is_convex_polygon(polygon_expected) # check if data is a strict convex polygon, if true, perform workflow else, DO WHAT?!\n",
    "print(binary_bool_expected)\n",
    "\n",
    "# Make figure to visualize anchor points and convex hull polygon\n",
    "for simplex_expected in hull_expected.simplices:\n",
    "    plt.plot(my_points_expected[simplex_expected, 0], my_points_expected[simplex_expected, 1], 'r--') # k-\n",
    "    plt.fill(my_points_expected[hull_expected.vertices, 0], my_points_expected[hull_expected.vertices, 1], c='yellow', alpha=0.01)\n",
    "plt.xlabel(\"MDS 1\")\n",
    "plt.ylabel(\"MDS 2\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Obtain the anchor points for n and n+1 scenarios using the expected stabilized solutions for both\n",
    "vertices_expected_index = hull_expected.vertices\n",
    "\n",
    "\n",
    "# Make sure the indexes of the anchor points from the data and check if the anchor points from scenario n is in scenario n+1 array as well\n",
    "expected_data_index_present = vertices_expected_index[np.isin(vertices_index, vertices_expected_index)]\n",
    "\n",
    "\n",
    "# Get anchors for n+1 scenario in expected stabilized solution\n",
    "case2_anchors_expected = my_points_expected[vertices_expected_index]\n",
    "anchors2_expected = np.column_stack((case2_anchors_expected[:, 0],case2_anchors_expected[:, 1],[0]*len(case2_anchors_expected)))\n",
    "\n",
    "\n",
    "# Recover the rotation and translation matrices R,t, respectively for the anchor points in n+1 to match anchors in the n case scenario\n",
    "R_anchors_expected, t_anchors_expected = fn.rigid_transform_3D(np.transpose(anchors2_expected), np.transpose(anchors1))\n",
    "\n",
    "# Compare the recovered R and t with the original by creating a new coordinate scheme via prior solutions of R, t\n",
    "new_coord_anchors_expected = (R_anchors_expected@np.transpose(anchors2_expected)) + t_anchors_expected\n",
    "\n",
    "# Find the rmse as an error check between estimated anchor points in n+1 scenario and anchor points in n scenario\n",
    "rmse_err_anchors_expected = fn.rmse(new_coord_anchors_expected, anchors1)\n",
    "print(rmse_err_anchors_expected)\n",
    "\n",
    "# Create a convex hull polygon of the expected stabilized anchor points. Set this as an assertion!\n",
    "new_coordanchor_expected = np.transpose(new_coord_anchors_expected[:2,:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Visualization of base case and stabilized solution\n",
    "fig , [ax0, ax1, ax2] = plt.subplots(1,3)\n",
    "\n",
    "# For base case anchors i.e. in n scenario # USE STABILIZED EXPECTATION ANCHORS\n",
    "ax0.scatter(anchors1[:,0], anchors1[:,1], marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "for index, label in enumerate(range(1,len(anchors1)+1)):\n",
    "    ax0.annotate(label, (anchors1[:,0][index]+0.05, anchors1[:,1][index]+0.05), size=10, style='italic')\n",
    "ax0.set_aspect('auto')\n",
    "ax0.set_title('Expected Stabilized Anchors from n sample scenario', size=12)\n",
    "ax0.set_xlabel('MDS 1',size=12)\n",
    "ax0.set_ylabel('MDS 2',size=12)\n",
    "ax0.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# For the realization anchors at n+1 scenario base case\n",
    "ax1.scatter(anchors2[:,0], anchors2[:,1], marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "for index, label in enumerate(range(1,len(anchors2)+1)):\n",
    "    ax1.annotate(label, (anchors2[:,0][index]+0.05, anchors2[:,1][index]+0.05), size=10, style='italic')\n",
    "ax1.set_aspect('auto')\n",
    "ax1.set_title('Anchors from n+1 sample scenario', size=12)\n",
    "ax1.set_xlabel('MDS 1',size=12)\n",
    "ax1.set_ylabel('MDS 2',size=12)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "## Visualize the expected stabilized anchor points\n",
    "ax2.scatter(new_coordanchor_expected[:,0], new_coordanchor_expected[:,1],  marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "for index, label in enumerate(range(1,len(new_coordanchor_expected[:,0])+1)):\n",
    "    ax2.annotate(label, (new_coordanchor_expected[:,0][index]+0.05, new_coordanchor_expected[:,1][index]+0.05), size=10, style='italic')\n",
    "ax2.set_aspect('auto')\n",
    "ax2.set_title('Expected stabilized anchor solution', size=12)\n",
    "ax2.set_xlabel('MDS 1',size=12)\n",
    "ax2.set_ylabel('MDS 2',size=12)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=1.3, wspace=0.25, hspace=0.3)\n",
    "plt.savefig( 'Expected Anchor sets & Stabilized Anchor set Solution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the R and t gotten from the expected stabilized anchor solution, and apply it to all samples in the n+1 scenario.\n",
    "my_points_expected_anchors = np.column_stack((my_points_expected[:, 0],my_points_expected[:, 1],[0]*len(my_points_expected)))\n",
    "new_coord_alldata_expected = (R_anchors_expected@np.transpose(my_points_expected_anchors)) + t_anchors_expected\n",
    "new_coord_alldata_expected_ = np.transpose(new_coord_alldata_expected[:2,:])\n",
    "\n",
    "\n",
    "# Visualize the stabilized n+1 scenario for all samples\n",
    "plt.scatter(new_coord_alldata_expected_[:n2-1,0], new_coord_alldata_expected_[:n2-1,1], marker='o', s=50, color='blue', edgecolors=\"black\")\n",
    "plt.scatter(new_coord_alldata_expected_[n2-1,0], new_coord_alldata_expected_[n2-1,1], marker='*', s=90, color='k', edgecolors=\"black\")\n",
    "\n",
    "for index, label in enumerate(range(1,len(new_coord_alldata_expected_[:,0])+1)):\n",
    "    plt.annotate(label, (new_coord_alldata_expected_[:,0][index]+0.05, new_coord_alldata_expected_[:,1][index]+0.05), size=8, style='italic')\n",
    "plt.xlabel(\"MDS 1\")\n",
    "plt.ylabel(\"MDS 2\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Let's check our model:\n",
    "\n",
    "* we will calculate the original and projected pairwise distances between all the samples\n",
    "\n",
    "* we will cross plot the original vs the projects pairwise distances\n",
    "\n",
    "* we will plot the distribution of the ratio between projects / original pariwise distances\n",
    "\n",
    "Now, let's calculate the distortion between original and projected pairwise distances between all samples. NEED TO UNDERSTAND INTERPRETATION OF STRESS HERE!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grab the expected stabilized solution of n+1 scenario\n",
    "stabilized_expected_proj_E = new_coord_alldata_expected_.copy()\n",
    "\n",
    "# insert distortion a loss function computation\n",
    "dists_expected = euclidean_distances(actual_data2, squared=False).ravel()\n",
    "nonzero_expected = dists_expected != 0   # select only non-identical samples pairs\n",
    "dists_expected = dists_expected[nonzero_expected]\n",
    "projected_dists_expected = euclidean_distances(stabilized_expected_proj_E, squared=False).ravel()[nonzero2]\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(dists_expected,projected_dists_expected,c='red',alpha=0.2,edgecolor = 'black')\n",
    "plt.arrow(0,0,200,200,width=0.02,color='black',head_length=0.0,head_width=0.0)\n",
    "plt.xlim(0,15); plt.ylim(0,15)\n",
    "plt.xlabel(\"Pairwise Distance: original space\")\n",
    "plt.ylabel(\"Pairwise Distance: projected space\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 components\")\n",
    "\n",
    "rates_expected = projected_dists_expected / dists_expected\n",
    "print(\"Distance Ratio, mean: %0.4f, standard deviation %0.4f.\" % (np.mean(rates_expected), np.std(rates_expected)))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(rates_expected, bins=50, range=(0.5, 1.5),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Distance Ratio: projected / original\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 Components\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(dists_expected, bins=50, range=(0., 15.),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Pairwise Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Original Data\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(projected_dists_expected, bins=50, range=(0., 15.),color = 'red', alpha = 0.2, edgecolor='k')\n",
    "plt.xlabel(\"Pairwise Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Pairwise Distance: Projected to 2 Components\")\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.7, top=2.3, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The average, minimum, and maximum stress values for the realizations in the n-scenario are \", round(np.mean(norm_stress),5) , round(np.min(norm_stress),5)\n",
    "      , round(np.max(norm_stress),5))\n",
    "\n",
    "print(\"The average, minimum, and maximum stress values for the realizations in the n-scenario are \", round(np.mean(norm_stress2),5) , round(np.min(norm_stress2),5)\n",
    "      , round(np.max(norm_stress2),5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T05:43:35.412108Z",
     "start_time": "2022-08-18T05:43:35.403071Z"
    }
   },
   "source": [
    "## Ademide (Midé) Mabadeje, Ph.D.Candidate, University of Texas at Austin \n",
    "\n",
    "\n",
    "Midé is currently working on Geostatistics – spatial data analytics, where she is creating novel workflows and data-driven algorithms to debias and reduce uncertainty in subsurface prediction models. Her Ph.D. research revolves around creating new data analytics and ML workflows to address various sources of spatial and/or subsurface bias to obtain representative sampling to impact decision making and economic development. \n",
    "\n",
    "For more about Midé check out these links:\n",
    "\n",
    "####  [GitHub](https://github.com/Mide478) | [GoogleScholar](https://scholar.google.com/citations?user=9nksjzQAAAAJ&hl=en&oi=ao)  | [LinkedIn](https://www.linkedin.com/in/ademidemabadeje)\n",
    "\n",
    "\n",
    "I am open to internships and collaborations in related research areas and disciplines underlined with spatial statistics. I can be reached at ademidemabadeje@austin.utexas.edu.\n",
    "\n",
    "## Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "We hope this was helpful,\n",
    "\n",
    "*Midé & Michael*\n",
    "\n",
    "\n",
    "\n",
    "Midé Mabadeje, Graduate Research Assistant The Hildebrand Department of Petroleum and Geosystems Engineering, The University of Texas at Austin\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1. To measure distortion between same samples in n, and n+1 samples over the same number of realizations, we find the stress, and make a figure of stress versus n tried for n=[1:N] and also compute the distortion ratio,  n+1/n. We hypothesize that as n increases, the plot of distortion version is linear wrt n+1 samples, however as n reduces, distortion becomes significant and it follows an exponential form.'\n",
    "\n",
    "\n",
    "2. Repeat for N on support [10, 100]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. Demonstrate variable, adjust the magnitude of predictors using weights (imposes prior knowledge) to see how projections and stabilizations change our inferences/solutions. Importance of invariant solutions in LD spaces and mappings for interpretation, repeatability ease etc. via Rigid Transforms.\n",
    "\n",
    "\n",
    "5. Repeat all above on Duvernay dataset. <br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
